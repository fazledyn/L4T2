{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy pandas pickle\n",
    "# %pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Conv2D:\n",
    "    \"\"\"\n",
    "    An implementation of the convolutional layer. We convolve the input with out_channels different filters\n",
    "    and each filter spans all channels in the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0):\n",
    "        \"\"\"\n",
    "        :param in_channels: the number of channels of the input data\n",
    "        :param out_channels: the number of channels of the output(aka the number of filters applied in the layer)\n",
    "        :param kernel_size: the specified size of the kernel(both height and width)\n",
    "        :param stride: the stride of convolution\n",
    "        :param padding: the size of padding. Pad zeros to the input with padding size.\n",
    "        \"\"\"\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.cache = None\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        self.weight = 1e-3 * np.random.randn(self.out_channels, self.in_channels,  self.kernel_size, self.kernel_size)\n",
    "        self.bias = np.zeros(self.out_channels)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"ConvolutionLayer\"\n",
    "\n",
    "    def __init__(self, n_filter, kernel_size, stride, padding):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_filter = n_filter\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "\n",
    "        self.cache = None\n",
    "    \n",
    "    def _get_windows(self, input, output_size, kernel_size, padding=0, stride=1, dilate=0):\n",
    "        working_input = input\n",
    "        working_pad = padding\n",
    "        # dilate the input if necessary\n",
    "        if dilate != 0:\n",
    "            working_input = np.insert(working_input, range(1, input.shape[2]), 0, axis=2)\n",
    "            working_input = np.insert(working_input, range(1, input.shape[3]), 0, axis=3)\n",
    "\n",
    "        # pad the input if necessary\n",
    "        if working_pad != 0:\n",
    "            working_input = np.pad(working_input, pad_width=((0,), (0,), (working_pad,), (working_pad,)), mode='constant', constant_values=(0.,))\n",
    "\n",
    "        in_b, in_c, out_h, out_w = output_size\n",
    "        out_b, out_c, _, _ = input.shape\n",
    "        batch_str, channel_str, kern_h_str, kern_w_str = working_input.strides\n",
    "\n",
    "        return np.lib.stride_tricks.as_strided(\n",
    "            working_input,\n",
    "            (out_b, out_c, out_h, out_w, kernel_size, kernel_size),\n",
    "            (batch_str, channel_str, stride * kern_h_str, stride * kern_w_str, kern_h_str, kern_w_str)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        The forward pass of convolution\n",
    "        :param x: input data of shape (N, C, H, W)\n",
    "        :return: output data of shape (N, self.out_channels, H', W') where H' and W' are determined by the convolution\n",
    "                 parameters.\n",
    "        \"\"\"\n",
    "        n, c, h, w = input.shape\n",
    "\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.randn(self.n_filter, c, self.kernel_size, self.kernel_size) / np.sqrt(2 / (self.kernel_size * self.kernel_size * c))\n",
    "        if self.biases is None:\n",
    "            self.biases = np.random.randn(self.n_filter)\n",
    "\n",
    "\n",
    "        out_h = (h - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "        out_w = (w - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "\n",
    "        windows = self._get_windows(input, (n, c, out_h, out_w), self.kernel_size, self.padding, self.stride)\n",
    "\n",
    "        out = np.einsum('bihwkl,oikl->bohw', windows, self.weights)\n",
    "\n",
    "        # add bias to kernels\n",
    "        out += self.biases[None, :, None, None]\n",
    "\n",
    "        self.cache = input, windows\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout, learning_rate):\n",
    "        \"\"\"\n",
    "        The backward pass of convolution\n",
    "        :param dout: upstream gradients\n",
    "        :return: dx, dw, and db relative to this module\n",
    "        \"\"\"\n",
    "        x, windows = self.cache\n",
    "\n",
    "        padding = self.kernel_size - 1 if self.padding == 0 else self.padding\n",
    "\n",
    "        dout_windows = self._get_windows(dout, x.shape, self.kernel_size, padding=padding, stride=1, dilate=self.stride - 1)\n",
    "        # rot_kern = np.rot90(self.weights, 2, axes=(2, 3))\n",
    "        rot_kern = np.rot90(self.weights, 2, axes=(2, 3))\n",
    "\n",
    "        db = np.sum(dout, axis=(0, 2, 3))\n",
    "        dw = np.einsum('bihwkl,bohw->oikl', windows, dout)\n",
    "        dx = np.einsum('bohwkl,oikl->bihw', dout_windows, rot_kern)\n",
    "\n",
    "        # return db, dw, dx\n",
    "        self.weights -= learning_rate * dw\n",
    "        self.biases -= learning_rate * db\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # def __forward(self, input):        \n",
    "    #     batch_size, n_channel, height, width = input.shape\n",
    "    #     output_shape = (batch_size, self.n_filter, int((height - self.kernel_size + 2*self.padding)/self.stride + 1), int((width - self.kernel_size + 2*self.padding)/self.stride + 1))\n",
    "    #     output = np.zeros(output_shape)\n",
    "\n",
    "    #     if self.weights is None:\n",
    "    #         self.weights = np.random.randn(self.n_filter, n_channel, self.kernel_size, self.kernel_size) / np.sqrt(2 / (self.kernel_size * self.kernel_size * n_channel))\n",
    "    #     if self.biases is None:\n",
    "    #         self.biases = np.random.randn(self.n_filter)\n",
    "\n",
    "    #     if self.padding > 0:\n",
    "    #         input = np.pad(input, ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)), 'constant')\n",
    "        \n",
    "    #     for b in range(batch_size):\n",
    "    #         for c in range(self.n_filter):\n",
    "    #             for h in range(height):\n",
    "    #                 for w in range(width):\n",
    "    #                     output[b, c, h, w] = np.sum(input[b, :, h*self.stride :h*self.stride + self.kernel_size, w*self.stride : w*self.stride + self.kernel_size] * self.filters[c, :, :, :]) + self.biases[c]\n",
    "\n",
    "    #     return output\n",
    "\n",
    "\n",
    "    # def __backward(self, output, learning_rate):\n",
    "    #     # perform back propagation for convolution\n",
    "\n",
    "    #     batch_size, n_channel, height, width = output.shape\n",
    "    #     input_shape = (batch_size, n_channel, height, width)\n",
    "    #     input = np.zeros(input_shape)\n",
    "\n",
    "    #     if self.padding > 0:\n",
    "    #         output = np.pad(output, ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)), 'constant')\n",
    "\n",
    "    #     for b in range(batch_size):\n",
    "    #         for c in range(self.n_filter):\n",
    "    #             for h in range(height):\n",
    "    #                 for w in range(width):\n",
    "    #                     print(\"left.shape\")\n",
    "    #                     print(input[b, :, h*self.stride :h*self.stride + self.kernel_size, w*self.stride : w*self.stride + self.kernel_size].shape)                        \n",
    "\n",
    "    #                     print(\"self.filters.shape\")\n",
    "    #                     print(self.filters.shape)\n",
    "\n",
    "    #                     print(\"right 2.shape\")\n",
    "    #                     print(self.filters[c, :, :, :].shape)\n",
    "\n",
    "    #                     input[b, :, h*self.stride :h*self.stride + self.kernel_size, w*self.stride : w*self.stride + self.kernel_size] += output[b, c, h, w] * self.filters[:, c, :, :]\n",
    "    #                     self.filters[:, c, :, :] += learning_rate * output[b, c, h, w] * input[b, :, h*self.stride :h*self.stride + self.kernel_size, w*self.stride : w*self.stride + self.kernel_size]\n",
    "    #                     self.biases[c] += learning_rate * output[b, c, h, w]\n",
    "\n",
    "    #     return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUActivationLayer:\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"ReLUActivationLayer\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        return np.maximum(input, 0)\n",
    "\n",
    "    def backward(self, output, learning_rate):\n",
    "        return np.where(output > 0, 1, 0)\n",
    "\n",
    "\n",
    "class MaxPoolingLayer:\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"MaxPoolingLayer\"\n",
    "\n",
    "    def __init__(self, pool_size, stride):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        batch_size, n_channel, height, width = input.shape\n",
    "\n",
    "        output_h = int((height - self.pool_size)/self.stride + 1)\n",
    "        output_w = int((width  - self.pool_size)/self.stride + 1)\n",
    "\n",
    "        output_shape = (batch_size, n_channel, output_h, output_w)\n",
    "        output = np.zeros(output_shape)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c in range(n_channel):\n",
    "                for h in range(output_h):\n",
    "                    for w in range(output_w):\n",
    "                        output[b, c, h, w] = np.max(input[b, :, h*self.stride :h*self.stride + self.pool_size, w*self.stride : w*self.stride + self.pool_size])\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, output, learning_rate):\n",
    "        batch_size, n_channel, height, width = output.shape\n",
    "        input = np.zeros(self.input.shape)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c in range(n_channel):\n",
    "                for h in range(height):\n",
    "                    for w in range(width):\n",
    "                        input[b, c, h*self.stride :h*self.stride + self.pool_size, w*self.stride : w*self.stride + self.pool_size] = np.where(input[b, c, h*self.stride :h*self.stride + self.pool_size, w*self.stride : w*self.stride + self.pool_size] == np.max(input[b, c, h*self.stride :h*self.stride + self.pool_size, w*self.stride : w*self.stride + self.pool_size]), output[b, c, h, w], 0)\n",
    "\n",
    "        return input    \n",
    "\n",
    "class FlatteningLayer:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.input = None\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"FlatteningLayer\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return input.reshape(input.shape[0], -1)\n",
    "\n",
    "    def backward(self, output, learning_rate):\n",
    "        return output.reshape(self.input.shape)\n",
    "\n",
    "\n",
    "class DenseLayer:\n",
    "\n",
    "    def __init__(self, n_output):\n",
    "        self.n_output = n_output\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.input = None\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"DenseLayer\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        self.input = input\n",
    "        batch_size, n_input = input.shape\n",
    "\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.randn(n_input, self.n_output) / np.sqrt(n_input)\n",
    "        if self.biases is None:\n",
    "            self.biases = np.random.randn(self.n_output)\n",
    "\n",
    "        output = np.dot(input, self.weights) + self.biases\n",
    "        return output\n",
    "\n",
    "\n",
    "    def backward(self, output, learning_rate):\n",
    "            \n",
    "            batch_size, n_input = output.shape    \n",
    "            grad_weights = np.dot(self.input.T, output)/n_input\n",
    "            \n",
    "            grad_biases = np.mean(output, axis=0)\n",
    "            grad_input = np.dot(output, grad_weights.T)\n",
    "\n",
    "            self.weights -= learning_rate * grad_weights\n",
    "            self.biases -= learning_rate * grad_biases\n",
    "\n",
    "            return grad_input\n",
    "\n",
    "\n",
    "\n",
    "class SoftMaxLayer:\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"SoftMaxLayer\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        val = input - np.max(input, axis=1, keepdims=True)\n",
    "        val = np.exp(val) / np.exp(val).sum(axis=1, keepdims=True)\n",
    "        return val\n",
    "\n",
    "    def backward(self, output, learning_rate):\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def loss_prime(y_true, y_pred):\n",
    "    return 2*(y_pred - y_true)\n",
    "\n",
    "BASEDIR = \"../../../numta\"\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"Load the dataset from the base directory\"\"\"\n",
    "    \n",
    "    dataset = f\"{BASEDIR}/training-a.csv\"\n",
    "    df = pd.read_csv(dataset)\n",
    "    df = df[[\"filename\", \"digit\"]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_image(image_name):\n",
    "    \n",
    "    img = cv2.imread(f\"{BASEDIR}/training-a/{image_name}\")\n",
    "    img = cv2.resize(img, (64, 64))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.invert(img)\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    print(img)\n",
    "    return img\n",
    "\n",
    "# data = load_dataset()\n",
    "# data.head\n",
    "\n",
    "# X = data[\"filename\"].values\n",
    "# y = data[\"digit\"].values\n",
    "\n",
    "# load_image(X[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  (10, 4, 32, 32)\n",
      "Convolution done\n",
      "output shape:  (10, 5, 32, 32)\n",
      "ReLU done\n",
      "output shape:  (10, 5, 32, 32)\n",
      "MaxPooling done\n",
      "output shape:  (10, 5, 31, 31)\n",
      "Flattening done\n",
      "output shape:  (10, 4805)\n",
      "Dense done\n",
      "output shape:  (10, 10)\n",
      "Softmax done\n",
      "output shape:  (10, 10)\n",
      "******************************\n",
      "******************************\n",
      "Softmax backward done\n",
      "output shape:  (10, 10)\n",
      "Dense backward done\n",
      "output shape:  (10, 4805)\n",
      "Flattening backward done\n",
      "output shape:  (10, 5, 31, 31)\n",
      "MaxPooling backward done\n",
      "output shape:  (10, 5, 32, 32)\n",
      "ReLU backward done\n",
      "output shape:  (10, 5, 32, 32)\n",
      "Convolution backward done\n",
      "output shape:  (10, 4, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# batch_size, n_channel, height, width\n",
    "input_shape = (10, 4, 32, 32)\n",
    "input = np.random.randn(*input_shape)\n",
    "\n",
    "print(\"input shape: \", input.shape)\n",
    "\n",
    "# n_filter, filter_size, stride, padding\n",
    "con = ConvolutionLayer(n_filter=5, kernel_size=3, stride=1, padding=1)\n",
    "relu = ReLUActivationLayer()\n",
    "max = MaxPoolingLayer(pool_size=2, stride=1)\n",
    "flat = FlatteningLayer()\n",
    "dens = DenseLayer(n_output=10)\n",
    "smax = SoftMaxLayer()\n",
    "\n",
    "output = con.forward(input)\n",
    "print(\"Convolution done\")\n",
    "print(\"output shape: \", output.shape)\n",
    "\n",
    "output = relu.forward(output)\n",
    "print(\"ReLU done\")\n",
    "print(\"output shape: \", output.shape)\n",
    "\n",
    "output = max.forward(output)\n",
    "print(\"MaxPooling done\")\n",
    "print(\"output shape: \", output.shape)\n",
    "\n",
    "output = flat.forward(output)\n",
    "print(\"Flattening done\")\n",
    "print(\"output shape: \", output.shape)\n",
    "\n",
    "output = dens.forward(output)\n",
    "print(\"Dense done\")\n",
    "print(\"output shape: \", output.shape)\n",
    "\n",
    "output = smax.forward(output)\n",
    "print(\"Softmax done\")\n",
    "print(\"output shape: \", output.shape)\n",
    "\n",
    "print(\"*\" * 30)\n",
    "print(\"*\" * 30)\n",
    "\n",
    "learning_rate = 0.1\n",
    "output = smax.backward(output, learning_rate)\n",
    "print(\"Softmax backward done\")\n",
    "print(\"output shape: \", output.shape)\n",
    "\n",
    "output = dens.backward(output, learning_rate)\n",
    "print(\"Dense backward done\")\n",
    "print(\"output shape: \", output.shape)\n",
    "\n",
    "output = flat.backward(output, learning_rate)\n",
    "print(\"Flattening backward done\")\n",
    "print(\"output shape: \", output.shape)\n",
    "\n",
    "output = max.backward(output, learning_rate)\n",
    "print(\"MaxPooling backward done\")\n",
    "print(\"output shape: \", output.shape)\n",
    "\n",
    "output = relu.backward(output, learning_rate)\n",
    "print(\"ReLU backward done\")\n",
    "print(\"output shape: \", output.shape)\n",
    "\n",
    "output = con.backward(dout=output, learning_rate=learning_rate)\n",
    "print(\"Convolution backward done\")\n",
    "print(\"output shape: \", output.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
