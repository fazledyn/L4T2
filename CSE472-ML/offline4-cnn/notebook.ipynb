{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy pandas pickle\n",
    "# %pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "class ConvolutionLayer:\n",
    "\n",
    "    def __init__(self, n_filter, filter_size, stride, padding):\n",
    "        self.filter_size = filter_size\n",
    "        self.n_filter = n_filter\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.filters = None\n",
    "        self.biases = None\n",
    "\n",
    "    \n",
    "    def forward(self, input):        \n",
    "        batch_size, n_channel, height, width = input.shape\n",
    "        output_shape = (batch_size, self.n_filter, int((height - self.filter_size + 2*self.padding)/self.stride + 1), int((width - self.filter_size + 2*self.padding)/self.stride + 1))\n",
    "        output = np.zeros(output_shape)\n",
    "\n",
    "        if self.filters is None:\n",
    "            self.filters = np.random.randn(self.n_filter, n_channel, self.filter_size, self.filter_size) / np.sqrt(2 / (self.filter_size * self.filter_size * n_channel))\n",
    "        if self.biases is None:\n",
    "            self.biases = np.random.randn(self.n_filter)\n",
    "\n",
    "        if self.padding > 0:\n",
    "            input = np.pad(input, ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)), 'constant')\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for c in range(self.n_filter):\n",
    "                for h in range(height):\n",
    "                    for w in range(width):\n",
    "                        output[b, c, h, w] = np.sum(input[b, :, h*self.stride :h*self.stride + self.filter_size, w*self.stride : w*self.stride + self.filter_size] * self.filters[c, :, :, :,]) + self.biases[c]\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "    # def backward(self, output, learning_rate):\n",
    "    #     # perform back propagation for convolution\n",
    "\n",
    "    #     batch_size, n_channel, height, width = output.shape\n",
    "    #     input_shape = (batch_size, n_channel, height, width)\n",
    "    #     input = np.zeros(input_shape)\n",
    "\n",
    "    #     if self.padding > 0:\n",
    "    #         output = np.pad(output, ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)), 'constant')\n",
    "        \n",
    "    #     for b in range(batch_size):\n",
    "    #         for c in range(self.n_filter):\n",
    "    #             for h in range(height):\n",
    "    #                 for w in range(width):\n",
    "    #                     input[b, :, h*self.stride :h*self.stride + self.filter_size, w*self.stride : w*self.stride + self.filter_size] += output[b, c, h, w] * self.filters[c, :, :, :,]\n",
    "    #                     self.filters[c, :, :, :,] -= learning_rate * output[b, c, h, w] * input[b, :, h*self.stride :h*self.stride + self.filter_size, w*self.stride : w*self.stride + self.filter_size]\n",
    "    #                     self.biases[c] -= learning_rate * output[b, c, h, w]\n",
    "        \n",
    "    #     return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUActivationLayer:\n",
    "\n",
    "    def forward(self, input):\n",
    "        return np.maximum(input, 0)\n",
    "\n",
    "    def backward(self, output, learning_rate):\n",
    "        return np.where(output > 0, 1, 0)\n",
    "\n",
    "class MaxPoolingLayer:\n",
    "\n",
    "    def __init__(self, pool_size, stride):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, n_channel, height, width = input.shape\n",
    "\n",
    "        output_h = int((height - self.pool_size)/self.stride + 1)\n",
    "        output_w = int((width  - self.pool_size)/self.stride + 1)\n",
    "\n",
    "        output_shape = (batch_size, n_channel, output_h, output_w)\n",
    "        output = np.zeros(output_shape)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c in range(n_channel):\n",
    "                for h in range(output_h):\n",
    "                    for w in range(output_w):\n",
    "                        output[b, c, h, w] = np.max(input[b, :, h*self.stride :h*self.stride + self.pool_size, w*self.stride : w*self.stride + self.pool_size])\n",
    "\n",
    "        return output\n",
    "\n",
    "    # def backward(self, output, learning_rate):\n",
    "    #     batch_size, n_channel, height, width = output.shape\n",
    "    #     input_shape = (batch_size, n_channel, height, width)\n",
    "    #     input = np.zeros(input_shape)\n",
    "\n",
    "    #     for b in range(batch_size):\n",
    "    #         for c in range(n_channel):\n",
    "    #             for h in range(height):\n",
    "    #                 for w in range(width):\n",
    "    #                     input[b, c, h*self.stride :h*self.stride + self.pool_size, w*self.stride : w*self.stride + self.pool_size] = np.where(input[b, c, h*self.stride :h*self.stride + self.pool_size, w*self.stride : w*self.stride + self.pool_size] == np.max(input[b, c, h*self.stride :h*self.stride + self.pool_size, w*self.stride : w*self.stride + self.pool_size]), output[b, c, h, w], 0)\n",
    "\n",
    "        # return input    \n",
    "        # \n",
    "\n",
    "class SoftMaxLayer:\n",
    "\n",
    "    def forward(self, input):\n",
    "        val = np.exp(input) / np.exp(input).sum()\n",
    "        return val\n",
    "\n",
    "    def fwd(self, input):\n",
    "        val = np.exp(input) / np.sum(np.exp(input))\n",
    "        return val\n",
    "\n",
    "    def backward(self, output, learning_rate):\n",
    "        return output\n",
    "\n",
    "\n",
    "class FlatteningLayer:\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.reshape(input.shape[0], -1)\n",
    "\n",
    "    def backward(self, output, learning_rate):\n",
    "        return output.reshape(output.shape[0], -1)\n",
    "\n",
    "\n",
    "class DenseLayer:\n",
    "\n",
    "    def __init__(self, n_output):\n",
    "        self.n_output = n_output\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        batch_size, n_input = input.shape\n",
    "\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.randn(n_input, self.n_output) / np.sqrt(n_input)\n",
    "        if self.biases is None:\n",
    "            self.biases = np.random.randn(self.n_output)\n",
    "\n",
    "        output = np.dot(input, self.weights) + self.biases\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[  9   1   6]\n",
      "   [  7   0 -10]\n",
      "   [  2   2  -2]]]]\n",
      "[[ 4.82816969  1.86551264  1.4269397  -4.88405721  3.64004782 -2.65252299\n",
      "   2.8304372  -2.77035312  1.18183446  1.18018694]]\n"
     ]
    }
   ],
   "source": [
    "# batch_size, n_channel, height, width\n",
    "# input = np.random.randn(*input_shape)\n",
    "input_shape = (1, 1, 3, 3)\n",
    "input = np.random.randint(-10, 10, size=input_shape)\n",
    "\n",
    "# n_filter, filter_size, stride, padding\n",
    "con = ConvolutionLayer(2, 1, 1, 1)\n",
    "max = MaxPoolingLayer(pool_size=2, stride=1)\n",
    "relu = ReLUActivationLayer()\n",
    "flat = FlatteningLayer()\n",
    "smax = SoftMaxLayer()\n",
    "dens = DenseLayer(10)\n",
    "\n",
    "# output = smax.forward(input)\n",
    "output = flat.forward(input)\n",
    "output = dens.forward(output)\n",
    "# output = relu.forward(input)\n",
    "# output = max.forward(input)\n",
    "# output = con.forward(input)\n",
    "\n",
    "\n",
    "print(input)\n",
    "# print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
