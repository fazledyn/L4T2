{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy pandas pickle\n",
    "# %pip install matplotlib\n",
    "# %pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import cv2\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"ConvolutionLayer\"\n",
    "\n",
    "    def __init__(self, n_filter, kernel_size, stride, padding):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_filter = n_filter\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "\n",
    "        self.cache = None\n",
    "    \n",
    "    def _get_windows(self, input, output_size, kernel_size, padding=0, stride=1, dilate=0):\n",
    "        working_input = input\n",
    "        working_pad = padding\n",
    "        # dilate the input if necessary\n",
    "        if dilate != 0:\n",
    "            working_input = np.insert(working_input, range(1, input.shape[2]), 0, axis=2)\n",
    "            working_input = np.insert(working_input, range(1, input.shape[3]), 0, axis=3)\n",
    "\n",
    "        # pad the input if necessary\n",
    "        if working_pad != 0:\n",
    "            working_input = np.pad(working_input, pad_width=((0,), (0,), (working_pad,), (working_pad,)), mode='constant', constant_values=(0.,))\n",
    "\n",
    "        in_b, in_c, out_h, out_w = output_size\n",
    "        out_b, out_c, _, _ = input.shape\n",
    "        batch_str, channel_str, kern_h_str, kern_w_str = working_input.strides\n",
    "\n",
    "        return np.lib.stride_tricks.as_strided(\n",
    "            working_input,\n",
    "            (out_b, out_c, out_h, out_w, kernel_size, kernel_size),\n",
    "            (batch_str, channel_str, stride * kern_h_str, stride * kern_w_str, kern_h_str, kern_w_str)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        n, c, h, w = input.shape\n",
    "\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.randn(self.n_filter, c, self.kernel_size, self.kernel_size) / np.sqrt(2 / (self.kernel_size * self.kernel_size * c))\n",
    "        if self.biases is None:\n",
    "            self.biases = np.random.randn(self.n_filter)\n",
    "\n",
    "        out_h = (h - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "        out_w = (w - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "\n",
    "        windows = self._get_windows(input, (n, c, out_h, out_w), self.kernel_size, self.padding, self.stride)\n",
    "        out = np.einsum('bihwkl,oikl->bohw', windows, self.weights)\n",
    "        out += self.biases[None, :, None, None]\n",
    "\n",
    "        self.cache = input, windows\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout, learning_rate):\n",
    "        x, windows = self.cache\n",
    "\n",
    "        padding = self.kernel_size - 1 if self.padding == 0 else self.padding\n",
    "        dout_windows = self._get_windows(dout, x.shape, self.kernel_size, padding=padding, stride=1, dilate=self.stride - 1)\n",
    "        rot_kern = np.rot90(self.weights, 2, axes=(2, 3))\n",
    "\n",
    "        db = np.sum(dout, axis=(0, 2, 3))\n",
    "        dw = np.einsum('bihwkl,bohw->oikl', windows, dout)\n",
    "        dx = np.einsum('bohwkl,oikl->bihw', dout_windows, rot_kern)\n",
    "\n",
    "        self.weights -= learning_rate * dw\n",
    "        self.biases -= learning_rate * db\n",
    "        return dx\n",
    "\n",
    "\n",
    "class ReLUActivationLayer:\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"ReLUActivationLayer\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        return np.maximum(input, 0)\n",
    "\n",
    "    def backward(self, output, learning_rate):\n",
    "        return np.where(output > 0, 1, 0)\n",
    "\n",
    "\n",
    "class MaxPoolingLayer:\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"MaxPoolingLayer\"\n",
    "\n",
    "    def __init__(self, pool_size, stride):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        batch_size, n_channel, height, width = input.shape\n",
    "\n",
    "        output_h = int((height - self.pool_size)/self.stride + 1)\n",
    "        output_w = int((width  - self.pool_size)/self.stride + 1)\n",
    "\n",
    "        output_shape = (batch_size, n_channel, output_h, output_w)\n",
    "        output = np.zeros(output_shape)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c in range(n_channel):\n",
    "                for h in range(output_h):\n",
    "                    for w in range(output_w):\n",
    "                        output[b, c, h, w] = np.max(input[b, :, h*self.stride :h*self.stride + self.pool_size, w*self.stride : w*self.stride + self.pool_size])\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, output, learning_rate):\n",
    "        batch_size, n_channel, height, width = output.shape\n",
    "        input = np.zeros(self.input.shape)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c in range(n_channel):\n",
    "                for h in range(height):\n",
    "                    for w in range(width):\n",
    "                        input[b, c, h*self.stride :h*self.stride + self.pool_size, w*self.stride : w*self.stride + self.pool_size] = np.where(input[b, c, h*self.stride :h*self.stride + self.pool_size, w*self.stride : w*self.stride + self.pool_size] == np.max(input[b, c, h*self.stride :h*self.stride + self.pool_size, w*self.stride : w*self.stride + self.pool_size]), output[b, c, h, w], 0)\n",
    "\n",
    "        return input    \n",
    "\n",
    "\n",
    "class FlatteningLayer:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.input = None\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"FlatteningLayer\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return input.reshape(input.shape[0], -1)\n",
    "\n",
    "    def backward(self, output, learning_rate):\n",
    "        return output.reshape(self.input.shape)\n",
    "\n",
    "\n",
    "class DenseLayer:\n",
    "\n",
    "    def __init__(self, n_output):\n",
    "        self.n_output = n_output\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.input = None\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"DenseLayer\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        self.input = input\n",
    "        batch_size, n_input = input.shape\n",
    "\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.randn(n_input, self.n_output) / np.sqrt(n_input)\n",
    "        if self.biases is None:\n",
    "            self.biases = np.random.randn(self.n_output)\n",
    "\n",
    "        output = np.dot(input, self.weights) + self.biases\n",
    "        return output\n",
    "\n",
    "\n",
    "    def backward(self, output, learning_rate):\n",
    "            \n",
    "            batch_size, n_input = output.shape    \n",
    "            grad_weights = np.dot(self.input.T, output)/n_input\n",
    "            \n",
    "            grad_biases = np.mean(output, axis=0)\n",
    "            grad_input = np.dot(output, grad_weights.T)\n",
    "\n",
    "            self.weights -= learning_rate * grad_weights\n",
    "            self.biases -= learning_rate * grad_biases\n",
    "\n",
    "            return grad_input\n",
    "\n",
    "\n",
    "class SoftMaxLayer:\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"SoftMaxLayer\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        val = input - np.max(input, axis=1, keepdims=True)\n",
    "        val = np.exp(val) / np.exp(val).sum(axis=1, keepdims=True)\n",
    "        return val\n",
    "\n",
    "    def backward(self, output, learning_rate):\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: (359, 2)\n",
      "Epoch: 1/10 Batch: 12/12\n",
      "Epoch: 2/10 Batch: 12/12\n",
      "Epoch: 3/10 Batch: 12/12\n",
      "Epoch: 4/10 Batch: 12/12\n",
      "Epoch: 5/10 Batch: 12/12\n",
      "Epoch: 6/10 Batch: 12/12\n",
      "Epoch: 7/10 Batch: 12/12\n",
      "Epoch: 8/10 Batch: 12/12\n",
      "Epoch: 9/10 Batch: 12/12\n",
      "Epoch: 10/10 Batch: 12/12\n",
      "Pickle File Dumped\n",
      "Pickle File Loaded\n",
      "Biases are same\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = \"../../../numta\"\n",
    "DATASET_NAME = \"training-b\"\n",
    "IMAGE_SHAPE = (28, 28)\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def backward(self, output, learning_rate) -> None:\n",
    "        for layer in reversed(self.layers):\n",
    "            output = layer.backward(output, learning_rate)\n",
    "    \n",
    "    def fit(self, X_train, y_train, learning_rate=0.01, epochs=10, batch_size=32):\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            n_batch = int(np.ceil(len(X_train)/batch_size))\n",
    "            for batch in range(n_batch):\n",
    "                #   reading batch of image_name from CSV and loading them as grayscale\n",
    "                batch_X = X_train[batch*batch_size : (batch+1)*batch_size]\n",
    "                batch_X = np.array([ 255 - load_image_as_grayscale(f\"{DATASET_DIR}/{DATASET_NAME}/{img_name}\") for img_name in batch_X ])\n",
    "\n",
    "                #   reading batch of labels from CSV and converting them to one-hot encoding\n",
    "                y_true = y_train[batch*batch_size : (batch+1)*batch_size]\n",
    "                y_true = np.array([ np.eye(10)[digit] for digit in y_true ])\n",
    "\n",
    "                y_pred = self.forward(batch_X)\n",
    "                y_hat = y_pred - y_true\n",
    "                self.backward(y_hat, learning_rate)\n",
    "            \n",
    "                print(f\"Epoch: {epoch+1}/{epochs} Batch: {batch+1}/{n_batch}\", end=\"\\r\")\n",
    "\n",
    "            print()\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)\n",
    "    \n",
    "\n",
    "# def cross_entropy_loss(y_true, y_pred):\n",
    "#     return np.sum(-1 * np.sum(y_true * np.log(y_pred), axis=0))\n",
    "\n",
    "def split_dataset(X, y, test_size=0.2, shuffle=True):\n",
    "    dataset = pd.DataFrame(X)\n",
    "    dataset[\"isoriginal\"] = y\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.sample(frac=1.0)\n",
    "\n",
    "    test_data = dataset.sample(frac=test_size).reset_index(drop=True)\n",
    "    train_data = dataset.drop(test_data.index).reset_index(drop=True)\n",
    "    \n",
    "    X_test = test_data.drop(columns=[\"isoriginal\"])\n",
    "    y_test = test_data[\"isoriginal\"].to_numpy()\n",
    "\n",
    "    X_train = train_data.drop(columns=[\"isoriginal\"])\n",
    "    y_train = train_data[\"isoriginal\"].to_numpy()\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    dataset = f\"{DATASET_DIR}/{DATASET_NAME}.csv\"\n",
    "    df = pd.read_csv(dataset)\n",
    "    df = df[[\"filename\", \"digit\"]]\n",
    "\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_image_as_grayscale(image_path):    \n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, IMAGE_SHAPE)\n",
    "    return np.array([img])\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    cnn = CNN()\n",
    "    cnn.add(ConvolutionLayer(n_filter=3, kernel_size=3, stride=1, padding=1))\n",
    "    cnn.add(ReLUActivationLayer())\n",
    "    cnn.add(MaxPoolingLayer(pool_size=2, stride=1))\n",
    "\n",
    "    cnn.add(ConvolutionLayer(n_filter=3, kernel_size=3, stride=1, padding=1))\n",
    "    cnn.add(ReLUActivationLayer())\n",
    "    cnn.add(MaxPoolingLayer(pool_size=2, stride=1))\n",
    "\n",
    "    cnn.add(FlatteningLayer())\n",
    "\n",
    "    cnn.add(DenseLayer(n_output=84))\n",
    "    cnn.add(DenseLayer(n_output=10))\n",
    "\n",
    "    cnn.add(SoftMaxLayer())\n",
    "    \n",
    "    dataset = load_dataset()\n",
    "    print(f\"Dataset Size: {dataset.shape}\")\n",
    "\n",
    "    X = dataset[\"filename\"].values\n",
    "    y = dataset[\"digit\"].values\n",
    "\n",
    "    # foo = load_image_as_grayscale(f\"{DATASET_DIR}/{DATASET_NAME}/{X[0]}\")\n",
    "    # print(f\"Image Shape: {foo.shape}\")\n",
    "    # print(255 - foo)\n",
    "\n",
    "    cnn.fit(X, y)\n",
    "\n",
    "    pickle.dump(cnn, open(\"cnn.pkl\", \"wb\"))\n",
    "    print(\"Pickle File Dumped\")\n",
    "\n",
    "    cnn_x = pickle.load(open(\"cnn.pkl\", \"rb\"))\n",
    "    print(\"Pickle File Loaded\")\n",
    "\n",
    "    # print(cnn_x.layers[0].biases)\n",
    "    # print(cnn_x.layers[0].biases)\n",
    "\n",
    "    if np.allclose(cnn.layers[0].biases, cnn_x.layers[0].biases):\n",
    "        print(\"Biases are same\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # batch_size, n_channel, height, width\n",
    "# input_shape = (10, 4, 32, 32)\n",
    "# input = np.random.randn(*input_shape)\n",
    "\n",
    "# print(\"input shape: \", input.shape)\n",
    "\n",
    "# # n_filter, filter_size, stride, padding\n",
    "# con = ConvolutionLayer(n_filter=5, kernel_size=3, stride=1, padding=1)\n",
    "# relu = ReLUActivationLayer()\n",
    "# max = MaxPoolingLayer(pool_size=2, stride=1)\n",
    "# flat = FlatteningLayer()\n",
    "# dens = DenseLayer(n_output=10)\n",
    "# smax = SoftMaxLayer()\n",
    "\n",
    "# output = con.forward(input)\n",
    "# print(\"Convolution done\")\n",
    "# print(\"output shape: \", output.shape)\n",
    "\n",
    "# output = relu.forward(output)\n",
    "# print(\"ReLU done\")\n",
    "# print(\"output shape: \", output.shape)\n",
    "\n",
    "# output = max.forward(output)\n",
    "# print(\"MaxPooling done\")\n",
    "# print(\"output shape: \", output.shape)\n",
    "\n",
    "# output = flat.forward(output)\n",
    "# print(\"Flattening done\")\n",
    "# print(\"output shape: \", output.shape)\n",
    "\n",
    "# output = dens.forward(output)\n",
    "# print(\"Dense done\")\n",
    "# print(\"output shape: \", output.shape)\n",
    "\n",
    "# output = smax.forward(output)\n",
    "# print(\"Softmax done\")\n",
    "# print(\"output shape: \", output.shape)\n",
    "\n",
    "# print(\"*\" * 30)\n",
    "# print(\"*\" * 30)\n",
    "\n",
    "# learning_rate = 0.1\n",
    "# output = smax.backward(output, learning_rate)\n",
    "# print(\"Softmax backward done\")\n",
    "# print(\"output shape: \", output.shape)\n",
    "\n",
    "# output = dens.backward(output, learning_rate)\n",
    "# print(\"Dense backward done\")\n",
    "# print(\"output shape: \", output.shape)\n",
    "\n",
    "# output = flat.backward(output, learning_rate)\n",
    "# print(\"Flattening backward done\")\n",
    "# print(\"output shape: \", output.shape)\n",
    "\n",
    "# output = max.backward(output, learning_rate)\n",
    "# print(\"MaxPooling backward done\")\n",
    "# print(\"output shape: \", output.shape)\n",
    "\n",
    "# output = relu.backward(output, learning_rate)\n",
    "# print(\"ReLU backward done\")\n",
    "# print(\"output shape: \", output.shape)\n",
    "\n",
    "# output = con.backward(dout=output, learning_rate=learning_rate)\n",
    "# print(\"Convolution backward done\")\n",
    "# print(\"output shape: \", output.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb633a12d11dc30df434576630afc913d986dae57cb6c8dd3204b88aa52721e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
