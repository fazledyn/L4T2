{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simply explain Gaussian Mixture Model\n",
    "\n",
    "A Gaussian mixture model (GMM) is a probabilistic model that assumes all the data points in a dataset are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. In other words, it is a generalization of the k-means clustering algorithm, which is also a special case of GMM, where the number of Gaussian distributions is set to one.\n",
    "\n",
    "The GMM algorithm is a model-based method for clustering, which means that it uses a generative model to describe the distribution of the data, and it uses the Expectation-Maximization (EM) algorithm to estimate the parameters of the Gaussian distributions from the data. Once the parameters are estimated, the GMM can be used to classify new data points into one of the clusters, or to generate new data points from one of the clusters.\n",
    "\n",
    "One of the main advantage of Gaussian mixture model, is that it allows for data points to have a probability of belonging to different clusters, which is useful when data points are not clearly separable into distinct clusters. It is commonly used in image segmentation, natural language processing and other fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GaussianMixtureModel:\n",
    "    def __init__(self, n_components, max_iter=100, tol=1e-3):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.means = None\n",
    "        self.covariances = None\n",
    "        self.weights = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize means, covariances, and weights\n",
    "        self.means = np.random.rand(self.n_components, n_features)\n",
    "        self.covariances = np.array([np.eye(n_features) for _ in range(self.n_components)])\n",
    "        self.weights = np.ones(self.n_components) / self.n_components\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            # Expectation step\n",
    "            responsibilities = self._e_step(X)\n",
    "            \n",
    "            # Maximization step\n",
    "            self._m_step(X, responsibilities)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.abs(self.weights.sum() - 1) < self.tol:\n",
    "                break\n",
    "    \n",
    "    def predict(self, X):\n",
    "        responsibilities = self._e_step(X)\n",
    "        return responsibilities.argmax(axis=1)\n",
    "    \n",
    "    def _e_step(self, X):\n",
    "        n_samples, _ = X.shape\n",
    "        responsibilities = np.zeros((n_samples, self.n_components))\n",
    "        for k in range(self.n_components):\n",
    "            responsibilities[:, k] = self.weights[k] * self._multivariate_gaussian(X, self.means[k], self.covariances[k])\n",
    "        responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n",
    "        return responsibilities\n",
    "    \n",
    "    def _m_step(self, X, responsibilities):\n",
    "        n_samples, n_features = X.shape\n",
    "        for k in range(self.n_components):\n",
    "            # Update weights\n",
    "            self.weights[k] = responsibilities[:, k].sum() / n_samples\n",
    "            \n",
    "            # Update means\n",
    "            weighted_sum = responsibilities[:, k].dot(X)\n",
    "            self.means[k] = weighted_sum / responsibilities[:, k].sum()\n",
    "            \n",
    "            # Update covariances\n",
    "            centered_data = X - self.means[k]\n",
    "            weighted_cov = np.dot(centered_data.T, responsibilities[:, k] * centered_data)\n",
    "            self.covariances[k] = weighted_cov / responsibilities[:, k].sum()\n",
    "    \n",
    "    def _multivariate_gaussian(self, X, mean, covariance):\n",
    "        n_samples = X.shape[0]\n",
    "        X = X - mean\n",
    "        return (2 * np.pi) ** (- X.shape[1] / 2) * np.linalg.det(covariance) ** -0.5 * \\\n",
    "            np.exp(-0.5 * np.sum(X @ np.linalg.inv(covariance) * X, axis=1))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb633a12d11dc30df434576630afc913d986dae57cb6c8dd3204b88aa52721e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
